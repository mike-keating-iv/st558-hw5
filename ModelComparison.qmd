---
title: "Model Comparison"
author: "Mike Keating"
format: pdf
editor: visual
---

# Model Comparison

## Task 1: Conceptual Questions

-   What is the purpose of using cross-validation when fitting a random forest model?

    Cross-validation is an important method to reduce risk of over fitting a model and to prevent data leakage. Since CV is performed on multiple data splits, it helps form a better understanding of a random forest models performance on unseen (i.e. real world) data.

-   Describe the bagged tree algorithm.

    Bagging stands for "bootstrap aggregation" in which multiple tree models are trained on bootstrapped data sets and the results aggregated to form the model's prediction. Bootstrapping is a sub sampling method that utilizes replacement, so each tree is trained on sub samples with varying compositions based on the original training set.

-   What is meant by a general linear model?

    A general linear model is a generic framework for predicting the response of a dependent variable on one or more independent variables (regressors, predictors). The response is a general linear combination of the predictors.

-   When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

    Adding an interaction term accounts for the situation where the value of one regressor can depend on the value of another regressor. This is represented in the formula as a product of these terms. This helps us model effects that are non-additive (say, they effect one type of person more than another, etc)

<!-- -->

-   Why do we split our data into a training and test set?

    In order to test the effectiveness of our model in real world situations, it needs to be tested on data that it was not trained on. Otherwise, the model may be over fit to the training data and the model would have poor generalization.

## Task 2: Data Prep

#### Packages and Data

```{r message=FALSE, warning=FALSE}
# Load Dependencies
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)
library(ggplot2)
```

#### Q1: Summary

```{r}
heart <- read_csv("data/heart.csv", show_col_types = FALSE)
summary(heart)
```

#### What type of variable (in R) is Heart Disease? Categorical or Quantitative?

Heart disease is a quantitative variable (double)

#### Does this make sense? Why or why not?

This does not make sense - as this variable should be treated as a factor, since it is a binary classification.

#### Q2: Alter HeartDisease Variable

```{r}
# Change heart disease to the correct type
# Perform some dataset cleanup

heart <- heart |> 
  mutate(HasHeartDisease = as_factor(HeartDisease)) |> 
  select(!c(ST_Slope, HeartDisease))

head(heart)
```

## Task 3: EDA

#### Q1: Plot

```{r}
library(ggplot2)
g <- ggplot(heart, aes(x = MaxHR, y = Age, color=HasHeartDisease)) + 
  geom_point(aes(alpha = 0.8)) +
  geom_smooth(method = "lm", se=FALSE) + 
  labs() + 
  theme_gray()
g
```

#### Q2: Additive vs Interaction

Since the above plot shows differing slopes for each factor of HasHeartDisease, an interaction model will be more suitable. However, it doesn't seem like the interaction is incredibly strong.

## Task 4: Testing and Training

#### Split into test and train sets

```{r}
# Set random seed
set.seed(101)

# Split into test and train sets
heart_split <- heart |> initial_split(prop=0.8)
train <- training(heart_split)
test <- testing(heart_split)

```

## Task 5: OLS and LASSO

#### Q1: Fit Interaction Model

ols_mlr_rec \<- recipe(Age \~ HasHeartDisease + MaxHR, data = train) \|\> step_normalize(all_numeric(), -all_outcomes()) \|\> step_interact(terms = \~ HasHeartDisease)

```{r}
# Fit model
ols_mlr <- lm(Age ~ HasHeartDisease + MaxHR + HasHeartDisease:MaxHR, data = train)
#summary(ols_mlr)

# Recipe
ols_mlr_rec <- recipe(Age ~ HasHeartDisease + MaxHR, data = train) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_interact(terms = ~ starts_with("HasHeart"):starts_with("MaxHR")) |>
  step_normalize(all_numeric_predictors()) 
```

```{r}
# Model
ols_mlr <- linear_reg() |> set_engine("lm")

# Workflow
ols_mlr_wfl <- workflow() |> add_recipe(ols_mlr_rec) |> add_model(ols_mlr)
ols_mlr_fit <- ols_mlr_wfl |> fit(train)

ols_mlr_fit |> tidy()
```

#### Q2: Find RMSE

```{r}
# Collect metrics/performance on the test set
ols_mlr_test_rmse <- ols_mlr_wfl |> 
  last_fit(heart_split) |> 
  collect_metrics() |> 
  filter(.metric == "rmse")
ols_mlr_test_rmse

```

#### Q3: LASSO

```{r}
# NOTE: The recipe/preprocessing steps are identical in this step to the ols model
LASSO_recipe <- recipe(Age ~ HasHeartDisease + MaxHR, data = train) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_interact(terms = ~ starts_with("HasHeart"):MaxHR) |>
  step_normalize(all_numeric_predictors()) 
  
LASSO_recipe
```

#### Q4: LASSO Model Selection

```{r}
# recall mixture = 1 sets to LASSO
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")

# Create workflow
LASSO_wkf <- workflow() |> add_recipe(LASSO_recipe) |> add_model(LASSO_spec)

# Cross-validation folds

heart_cv_folds <- vfold_cv(train, 5)

# Create grid object
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_cv_folds,
            grid = grid_regular(penalty(),
                               levels = 20))
# Display grid
LASSO_grid
```

```{r}
LASSO_grid |> collect_metrics() |> 
  filter(.metric == "rmse") |>
  ggplot(aes(penalty, mean, color = .metric)) + geom_line()
```

```{r}
# Select best model based on RMSE
lowest_rmse <- LASSO_grid |> select_best(metric = "rmse")

LASSO_final <- LASSO_wkf |> finalize_workflow(lowest_rmse) |>
  fit(train)
tidy(LASSO_final)

```

#### Q5: Model Expectations

Without looking at the RMSE calculations, I would expect the RMSE calculations to be roughly the same. This is because we already pre-selected only HasHeartDisease and MaxHR as predictors. The LASSO method would likely have a more dramatic effect on models with larger amount of initial predictors. I also imagine both models might reach a prediction plateau based on only these features.

#### Q6: Compare OLS and LASSO

```{r}
# Get rmse from our LASSO model
LASSO_test_rmse <- LASSO_wkf |> 
  finalize_workflow(lowest_rmse) |> 
  last_fit(heart_split) |> 
  collect_metrics() |> 
  filter(.metric == "rmse")

# Combine our metrics
rbind(ols_mlr_test_rmse, LASSO_test_rmse) |> 
  mutate(Model = c("OLS", "LASSO")) |> 
  select(Model, everything())
```

#### Q7: Explain RMSE Similarity

At the end of the day, the coefficients are still fairly similar and the LASSO method did not enact a large amount of shrinkage. The number of predictors was largely unchanged (none shrunk to exactly zero) and the models likely arrive at similar predictions.

## Task 6: Logistic Regression

```{r}

# First Model
# All predictors

# Use the same split as before
LR1_recipe <- recipe(HasHeartDisease ~ . , data = train) |>
  step_normalize(all_numeric_predictors())

# Second Model
# 
LR2_recipe <- recipe(HasHeartDisease ~ Age + Sex + Cholesterol, data = train) |>
  step_normalize(all_numeric_predictors())

# Spec shared between both models
LR_spec <- logistic_reg() |> 
  set_engine("glm")

# Create workflows
LR1_wkf <- workflow() |> 
  add_recipe(LR1_recipe) |> 
  add_model(LR_spec)

LR2_wkf <- workflow() |> 
  add_recipe(LR2_recipe) |> 
  add_model(LR_spec)

LR_cv_folds = vfold_cv(train, 5, 3) # 3 repeats

# Fit models
LR1_fit <- LR1_wkf |> 
  fit_resamples(LR_cv_folds, metrics = metric_set(accuracy, mn_log_loss))

LR2_fit <- LR2_wkf |> 
  fit_resamples(LR_cv_folds, metrics = metric_set(accuracy, mn_log_loss))

# Metrics
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics()) |> mutate(Model = c("Model 1", "Model 1", "Model 2", "Model 2")) |> select(Model, everything())
```

Model 1 (all predictors) performs better on both accuracy (0.811) and log loss (0419, lower is better).

```{r}
# Fit model 1
LR_final_fit <- LR1_wkf |> fit(data = train)
LR_preds <- predict(LR_final_fit, test) |> pull(.pred_class)

confusionMatrix(data = LR_preds, reference = test$HasHeartDisease)
```

Sensitivity is also known as recall and represents the true positive rate. This is how well we identify true positives (i.e. 1, or the patient has heart disease)

Specificity represents the true negative rate, or how well our model identifies true negatives.
